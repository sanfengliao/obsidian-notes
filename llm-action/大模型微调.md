å¾®è°ƒï¼ˆFine-Tunineï¼‰æ˜¯åœ¨é¢„è®­ç»ƒå¤§æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–åœºæ™¯è¿›è¡Œå°èŒƒå›´çš„è°ƒæ•´ä¼˜åŒ–ã€‚å®ƒçš„ç›®æ ‡æ˜¯åœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªå¤§æ¨¡å‹çš„å‰æä¸‹ï¼Œé€šè¿‡è°ƒæ•´éƒ¨åˆ†å‚æ•°æˆ–ç‰¹å®šæ¨¡å—ï¼Œå®ç°æ¨¡å‹å¯¹å…·ä½“ä»»åŠ¡çš„å¿«é€Ÿé€‚é…ã€‚
[å¤§æ¨¡å‹ç›¸å…³æŠ€æœ¯åŸç†ä»¥åŠå®æˆ˜ç»éªŒï¼ˆå¤§æ¨¡å‹å·¥ç¨‹åŒ–ã€å¤§æ¨¡å‹åº”ç”¨è½åœ°ï¼‰](https://github.com/liguodongiot/llm-action/tree/main)
# å…¨å‚æ•°å¾®è°ƒ
è°ƒæ•´æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œé€‚ç”¨äºæ•°æ®é‡å¤§çš„æƒ…å†µï¼Œä½†è®¡ç®—èµ„æºæ¶ˆè€—é«˜ã€‚ç”±äºå¤§è¯­è¨€æ¨¡å‹å‚æ•°è§„æ¨¡å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œè¿™ä½¿å¾—åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿›è¡Œå…¨é‡å¾®è°ƒå˜å¾—ä¸å¯è¡Œã€‚

# å‚æ•°é«˜æ•ˆå¾®è°ƒ

å¾®è°ƒå°‘é‡æˆ–é¢å¤–çš„æ¨¡å‹å‚æ•°ï¼Œå›ºå®šå¤§éƒ¨åˆ†é¢„è®­ç»ƒæ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ï¼ŒåŒæ—¶ï¼Œä¹Ÿèƒ½å®ç°ä¸å…¨é‡å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æ¯”å…¨é‡å¾®è°ƒæ•ˆæœæ›´å¥½ï¼Œå¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°åŸŸå¤–åœºæ™¯ã€‚
é«˜æ•ˆå¾®è°ƒæŠ€æœ¯å¯ä»¥ç²—ç•¥åˆ†ä¸ºä»¥ä¸‹ä¸‰å¤§ç±»ï¼šå¢åŠ é¢å¤–å‚æ•°ï¼ˆAï¼‰ã€é€‰å–ä¸€éƒ¨åˆ†å‚æ•°æ›´æ–°ï¼ˆSï¼‰ã€å¼•å…¥é‡å‚æ•°åŒ–ï¼ˆRï¼‰ã€‚è€Œåœ¨å¢åŠ é¢å¤–å‚æ•°è¿™ç±»æ–¹æ³•ä¸­ï¼Œåˆä¸»è¦åˆ†ä¸ºç±»é€‚é…å™¨ï¼ˆAdapter-likeï¼‰æ–¹æ³•å’Œè½¯æç¤ºï¼ˆSoft promptsï¼‰ä¸¤ä¸ªå°ç±»ã€‚
![alt text](assets/1740382924772.png)

å¸¸è§çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯æœ‰BitFitã€Prefix Tuningã€Prompt Tuningã€P-Tuningã€Adapter Tuningã€LoRAç­‰ï¼Œåç»­æ–‡ç« å°†å¯¹ä¸€äº›ä¸»æµçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•è¿›è¡Œè®²è§£

ä»¥ä¸‹æ˜¯å‡ ç§ä¸»æµçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuning, PEFTï¼‰æ–¹æ³•çš„ç®€è¦ä»‹ç»ï¼š
## å¸¸è§å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸€ï¼‰-èƒŒæ™¯ã€å‚æ•°é«˜æ•ˆå¾®è°ƒç®€ä»‹](https://zhuanlan.zhihu.com/p/635152813)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆäºŒï¼‰-BitFitã€Prefix Tuningã€Prompt Tuning](https://zhuanlan.zhihu.com/p/635686756)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸‰ï¼‰-P-Tuningã€P-Tuning v2](https://zhuanlan.zhihu.com/p/635848732)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆå››ï¼‰-Adapter TuningåŠå…¶å˜ä½“](https://zhuanlan.zhihu.com/p/636038478)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆäº”ï¼‰-LoRAã€AdaLoRAã€QLoRA](https://zhuanlan.zhihu.com/p/636215898)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆå…­ï¼‰-MAM Adapterã€UniPELT](https://zhuanlan.zhihu.com/p/636362246)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸ƒï¼‰-æœ€ä½³å®è·µã€æ€»ç»“](https://zhuanlan.zhihu.com/p/649755252)
### **1. BitFitï¼ˆBias-term Fine-Tuningï¼‰**
- **åŸç†**ï¼šä»…å¾®è°ƒæ¨¡å‹ä¸­çš„**åç½®å‚æ•°**ï¼ˆbias termsï¼‰ï¼Œå†»ç»“å…¶ä»–æ‰€æœ‰æƒé‡å‚æ•°ã€‚
- **ç‰¹ç‚¹**ï¼š
  - è®¡ç®—æˆæœ¬æä½ï¼Œä»…éœ€è®­ç»ƒçº¦0.1%çš„å‚æ•°é‡ã€‚
  - é€‚ç”¨äºç®€å•ä»»åŠ¡æˆ–èµ„æºå—é™åœºæ™¯ã€‚
  - åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸­è¡¨ç°æ¥è¿‘å…¨é‡å¾®è°ƒã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šè½»é‡çº§é€‚é…ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€‚
### **2. Prefix Tuning**
- **åŸç†**ï¼šåœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ **å¯è®­ç»ƒçš„å‰ç¼€å‘é‡**ï¼ˆprefix tokensï¼‰ï¼Œé€šè¿‡è°ƒæ•´è¿™äº›å‘é‡å¼•å¯¼æ¨¡å‹è¾“å‡ºã€‚
  - å‰ç¼€å‘é‡æ˜¯è¿ç»­çš„ï¼ˆéçœŸå®tokenï¼‰ï¼Œé€šè¿‡æ¢¯åº¦ä¼˜åŒ–å­¦ä¹ ã€‚
  - é€šå¸¸åº”ç”¨äºTransformerçš„æ¯ä¸€å±‚ã€‚
- **ç‰¹ç‚¹**ï¼š
  - å‚æ•°é«˜æ•ˆï¼ˆä»…è°ƒæ•´çº¦0.1%-1%çš„å‚æ•°é‡ï¼‰ã€‚
  - é€‚åˆç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚å¯¹è¯ã€æ‘˜è¦ï¼‰ã€‚
- **æ”¹è¿›**ï¼šä¸ç¦»æ•£çš„Promptä¸åŒï¼ŒPrefixæ˜¯è¿ç»­å¯å­¦ä¹ çš„å‘é‡ï¼Œä¼˜åŒ–æ›´ç¨³å®šã€‚
### **3. Prompt Tuning**
- **åŸç†**ï¼šåœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ **å¯å­¦ä¹ çš„æç¤ºå‘é‡**ï¼ˆsoft promptsï¼‰ï¼Œé€šè¿‡è°ƒæ•´è¿™äº›å‘é‡é€‚é…ä»»åŠ¡ã€‚
  - ä»…ä½œç”¨äºè¾“å…¥å±‚ï¼Œä¸æ¶‰åŠæ¨¡å‹å†…éƒ¨ç»“æ„ã€‚
  - æç¤ºå‘é‡çš„é•¿åº¦å’Œå‚æ•°å¯è°ƒã€‚
- **ç‰¹ç‚¹**ï¼š
  - å‚æ•°é‡æä½ï¼ˆé€šå¸¸ä¸º0.01%-0.1%ï¼‰ã€‚
  - å¯¹æ¨¡å‹è§„æ¨¡æ•æ„Ÿï¼Œå¤§æ¨¡å‹ï¼ˆå¦‚ç™¾äº¿å‚æ•°ï¼‰æ•ˆæœæ›´ä½³ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šæ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ä»»åŠ¡ã€‚

### **4. P-Tuningï¼ˆPrompt Tuningçš„å‡çº§ç‰ˆï¼‰**
- **åŸç†**ï¼šå¼•å…¥**è¿ç»­å¯å­¦ä¹ çš„æç¤ºå‘é‡**ï¼Œå¹¶é€šè¿‡å°å‹ç¥ç»ç½‘ç»œï¼ˆå¦‚LSTMæˆ–MLPï¼‰ç”Ÿæˆæ›´å¤æ‚çš„æç¤ºã€‚
  - è§£å†³ä¼ ç»Ÿç¦»æ•£æç¤ºï¼ˆhard promptï¼‰éš¾ä»¥ä¼˜åŒ–çš„é—®é¢˜ã€‚
- **ç‰¹ç‚¹**ï¼š
  - ä¼˜åŒ–æ›´ç¨³å®šï¼Œæ”¯æŒå¤æ‚ä»»åŠ¡ï¼ˆå¦‚çŸ¥è¯†æ¨ç†ï¼‰ã€‚
  - ç›¸æ¯”æ™®é€šPrompt Tuningï¼Œæ•ˆæœæ›´ä¼˜ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å¤æ‚æç¤ºç»“æ„çš„ä»»åŠ¡ï¼ˆå¦‚é—®ç­”ã€é€»è¾‘æ¨ç†ï¼‰ã€‚

### **5. Adapter Tuning**
- **åŸç†**ï¼šåœ¨Transformerå±‚çš„**å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰åæ’å…¥å°å‹é€‚é…å™¨æ¨¡å—**ï¼ˆAdapterï¼‰ã€‚
  - Adapteré€šå¸¸åŒ…å«é™ç»´ï¼ˆdown-projectionï¼‰å’Œå‡ç»´ï¼ˆup-projectionï¼‰çš„å…¨è¿æ¥å±‚ã€‚
  - ä»…è®­ç»ƒAdapterå‚æ•°ï¼Œå†»ç»“åŸæ¨¡å‹å‚æ•°ã€‚
- **ç‰¹ç‚¹**ï¼š
  - é€‚é…å™¨å‚æ•°é‡å°ï¼ˆçº¦1%-5%ï¼‰ï¼Œé€‚åˆé€å±‚é€‚é…ã€‚
  - å¯èƒ½ç•¥å¾®å¢åŠ æ¨ç†å»¶è¿Ÿï¼ˆéœ€ä¸²è¡Œè®¡ç®—é€‚é…å™¨ï¼‰ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šå¤šä»»åŠ¡å­¦ä¹ ã€å¤šè¯­è¨€é€‚é…ã€‚

### **6. LoRAï¼ˆLow-Rank Adaptationï¼‰**
- **åŸç†**ï¼šé€šè¿‡**ä½ç§©åˆ†è§£**è¿‘ä¼¼æƒé‡æ›´æ–°ï¼Œåœ¨åŸå§‹æƒé‡çŸ©é˜µæ—æ·»åŠ ä½ç§©çŸ©é˜µï¼ˆå¦‚Î”W = AÂ·Bï¼ŒAå’ŒBä¸ºä½ç§©çŸ©é˜µï¼‰ã€‚
  - ä»…è®­ç»ƒä½ç§©çŸ©é˜µå‚æ•°ï¼Œå†»ç»“åŸæ¨¡å‹æƒé‡ã€‚
  - è®­ç»ƒåå°†ä½ç§©çŸ©é˜µåˆå¹¶å›åŸæ¨¡å‹ï¼Œ**æ¨ç†æ—¶æ— é¢å¤–è®¡ç®—**ã€‚
- **ç‰¹ç‚¹**ï¼š
  - å‚æ•°é‡å°ï¼ˆé€šå¸¸ä¸º0.5%-2%ï¼‰ï¼Œè®¡ç®—é«˜æ•ˆã€‚
  - å‡ ä¹ä¸å¢åŠ æ¨ç†å»¶è¿Ÿï¼Œé€‚åˆå¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚GPT-3ã€LLaMAï¼‰ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šç”Ÿæˆä»»åŠ¡ã€å¤§è§„æ¨¡æ¨¡å‹å¾®è°ƒã€‚


| æ–¹æ³•            | æ ¸å¿ƒæ€æƒ³                     | å‚æ•°é‡å æ¯” | è®¡ç®—æˆæœ¬ | é€‚ç”¨åœºæ™¯               | ä¼˜ç‚¹                          | ç¼ºç‚¹                     |
|-----------------|----------------------------|------------|----------|------------------------|-------------------------------|--------------------------|
| **BitFit**      | ä»…å¾®è°ƒåç½®å‚æ•°               | ~0.1%      | æä½      | ç®€å•åˆ†ç±»ä»»åŠ¡           | æç®€å®ç°ï¼Œèµ„æºå‹å¥½             | ä»»åŠ¡é€‚é…èƒ½åŠ›æœ‰é™         |
| **Prefix Tuning**| æ·»åŠ å¯å­¦ä¹ å‰ç¼€å‘é‡           | 0.1%-1%    | ä¸­ç­‰      | ç”Ÿæˆä»»åŠ¡               | æ•ˆæœæ¥è¿‘å…¨é‡å¾®è°ƒ               | ä¼˜åŒ–éš¾åº¦è¾ƒé«˜             |
| **Prompt Tuning**| å­¦ä¹ è¾“å…¥å±‚æç¤ºå‘é‡           | 0.01%-0.1% | ä½        | ç”Ÿæˆ/åˆ†ç±»ä»»åŠ¡          | å‚æ•°æå°‘ï¼Œé€‚åˆå¤§æ¨¡å‹           | ä¾èµ–æ¨¡å‹è§„æ¨¡             |
| **P-Tuning**    | é€šè¿‡ç¥ç»ç½‘ç»œç”Ÿæˆè¿ç»­æç¤º       | 0.1%-1%    | ä¸­ç­‰      | å¤æ‚æ¨ç†ä»»åŠ¡           | æç¤ºä¼˜åŒ–æ›´çµæ´»                 | å®ç°å¤æ‚åº¦è¾ƒé«˜           |
| **Adapter**     | æ’å…¥å°å‹é€‚é…å™¨æ¨¡å—           | 1%-5%      | ä¸­ç­‰      | å¤šä»»åŠ¡/å¤šè¯­è¨€åœºæ™¯      | æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•           | å¯èƒ½å¢åŠ æ¨ç†å»¶è¿Ÿ         |
| **LoRA**        | ä½ç§©åˆ†è§£è¿‘ä¼¼æƒé‡æ›´æ–°         | 0.5%-2%    | ä½        | å¤§è§„æ¨¡æ¨¡å‹å¾®è°ƒ         | é«˜æ•ˆä¸”æ— æ¨ç†å¼€é”€               | éœ€è®¾è®¡ä½ç§©ç»“æ„           |
# å¾®è°ƒæµç¨‹
```mermaid
graph TD
    A[æ˜ç¡®ä»»åŠ¡ç›®æ ‡] --> B[æ•°æ®å‡†å¤‡]
    B --> C[é€‰æ‹©åŸºåº§æ¨¡å‹]
    C --> D[ç¡®å®šå¾®è°ƒæ–¹æ³•]
    D --> E[è®­ç»ƒé…ç½®]
    E --> F[æ¨¡å‹è¯„ä¼°]
    F --> G[éƒ¨ç½²ä¼˜åŒ–]
```

## Loraå¾®è°ƒ

**LoRA**(Low-Rank Adaptation)æ˜¯ä¸€ç§é«˜æ•ˆçš„å¤§æ¨¡å‹**PEFT**å¾®è°ƒæŠ€æœ¯ï¼Œå®ƒæ˜¯é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„å…³é”®å±‚ï¼ˆå¦‚å…¨è¿æ¥å±‚å’Œè‡ªæ³¨æ„åŠ›å±‚ï¼‰ä¹‹é—´æ·»åŠ ä½ç§©çŸ©é˜µæ¥å®Œæˆå¾®è°ƒã€‚è¿™äº›ä½ç§©çŸ©é˜µçš„å¼•å…¥ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€æ”¹å˜åŸæœ‰çš„å¤§é‡å‚æ•°ã€‚ç”±äºä½ç§©çŸ©é˜µçš„å‚æ•°æ•°é‡è¿œå°äºåŸæœ‰å±‚çš„å‚æ•°æ•°é‡ï¼Œè¿™å°±å¤§å¤§å‡å°‘äº†éœ€è¦è®­ç»ƒçš„å‚æ•°æ€»æ•°ã€‚
æ›´å¤šç»†èŠ‚å‚è€ƒ
* [æ·±å…¥æµ…å‡ºLora](https://zhuanlan.zhihu.com/p/650197598)
* [ä½é…æœºå™¨ä¹Ÿèƒ½å¾®è°ƒå¤§æ¨¡å‹ï¼Ÿæ‰‹æŠŠæ‰‹æ•™ä½ å®æˆ˜æŠ€å·§ï¼](https://mp.weixin.qq.com/s?__biz=MzA3NzE0MjAwMg==&mid=2452715024&idx=1&sn=4917ef9937647cd9bdb6d266d2f33620&chksm=89dcca5eb8513662d3f2d5e9ca366b10b12630f70b464c405139b3f72b738ec9bf29b3295f3e#rd)
* [åŸºäº Qwen2 å¤§æ¨¡å‹å¾®è°ƒæŠ€æœ¯è¯¦ç»†æ•™ç¨‹ï¼ˆLoRA å‚æ•°é«˜æ•ˆå¾®è°ƒå’Œ SwanLab å¯è§†åŒ–ç›‘æ§ï¼‰](https://www.cnblogs.com/obullxl/p/18312594/NTopic2024071801)
* [ä»é›¶å¼€å§‹æ‰‹å†™å¾®è°ƒä»£ç ï¼šå¦‚ä½•ç”¨æœ¬åœ°deepseekæ¨¡å‹åœ¨è‡ªå·±æ„å»ºçš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒ](https://mp.weixin.qq.com/s?__biz=MzU0NDkyMzg3Mg==&mid=2247483835&idx=1&sn=2e010f62f98ccde903ba81262e252bdb&chksm=fa5b6d53e3d525992829812cc4ea7653251cdb564b0752af69e54de098397380fc4c1143aacf#rd)
* [DeepSeekå¤§æ¨¡å‹å¾®è°ƒï¼Œå®¶æ•™å¼å…¨æµç¨‹å®æˆ˜æŒ‡å—ï¼](https://mp.weixin.qq.com/s?__biz=MzU3Nzc0MzE3MA==&mid=2247484640&idx=1&sn=ea0605378dc056460379c8bedcfa773f&chksm=fc12b7922060120563085c0111b74b6555d91887ebf365bf8d3db1ae1a2d91fe0cd6b018b923#rd)
* [æ‰‹æŠŠæ‰‹æ•™å­¦ï¼ŒDeepSeek-R1å¾®è°ƒå…¨æµç¨‹æ‹†è§£](https://www.cnblogs.com/shanren/p/18707513)
* [å¤§è¯­è¨€æ¨¡å‹å¸¸ç”¨å¾®è°ƒä¸åŸºäºSFTå¾®è°ƒDeepSeek R1æŒ‡å—](https://www.ewbang.com/community/article/details/1000168010.html)

### DeepSeek

#### åŠ è½½æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

model_name = "deepseek-ai/deepseek-llm-7b-base"
# Configure 4-bit quantization
# ä½¿ç”¨ 4 ä½é‡åŒ–ä½¿å¤§å‹æ¨¡å‹ä¸æœ‰é™çš„ GPU å†…å­˜å…¼å®¹ï¼š
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16  # Use float16 for faster computation
)
# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    quantization_config=bnb_config, 
    device_map="auto"
)
```

#### åŠ è½½æ•°æ®é›†

ä½¿ç”¨ IMDB æ•°æ®é›†å¯¹ DeepSeek LLM è¿›è¡Œæƒ…ç»ªåˆ†ç±»å¾®è°ƒï¼ŒåŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ï¼Œå¹¶å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†

```python

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("imdb")

def tokenize_function(examples):
    inputs = tokenizer(
        examples["text"], 
        truncation=True, 
        padding="max_length", 
        max_length=512
    )
    inputs["labels"] = inputs["input_ids"].copy()
    return inputs

# é¢„å¤„ç†æ•°æ®é›†
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(500))
small_test_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(100))
# Print a sample tokenized entry
print("Tokenized Sample:")
print(small_train_dataset[0])

```

#### é…ç½®Lora
```python
# Apply LoRA for memory-efficient fine-tuning
lora_config = LoraConfig(
    r=8,  # Low-rank adaptation size
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Apply LoRA to attention layers
    lora_dropout=0.05,
    bias="none"
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

#### è®¾ç½®è®­ç»ƒå‚æ•°
```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-4,  # Lower learning rate for LoRA fine-tuning
    per_device_train_batch_size=1,  # Reduce batch size for memory efficiency
    gradient_accumulation_steps=8,  # Simulate larger batch size
    num_train_epochs=0.5,
    weight_decay=0.01,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=50,
    fp16=True,  # Mixed precision training
)
```

#### åˆå§‹åŒ–è®­ç»ƒæœºå™¨å¹¶å¾®è°ƒè¯•
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_test_dataset,
)
print("ğŸš€ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

print("ğŸš€ å¼€å§‹å¾®è°ƒ")
trainer.train()


trainer.save_model("./my_deepseek")
tokenizer.save_pretrained("./my_deepseek")
print("æ¨¡å‹å·²ä¿å­˜åˆ° ./my_deepseek")
```

### Qwen2
#### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨(ç¡®è®¤åŸºåº§æ¨¡å‹)

```python
# æ¨¡å‹è·¯å¾„
TARGET_MODEL = ''

# ç¡®è®¤è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# é…ç½®8bité‡åŒ–ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘å¯¹è®¾å¤‡æ€§èƒ½çš„æ¶ˆè€—ã€‚
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
)

# åŠ è½½åˆ†è¯æœºå™¨
tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)
tokenizer.padding_side = 'right'
tokenizer.pad_token = tokenizer.eos_token  # æ·»åŠ è¿™ä¸€è¡Œï¼Œè®¾ç½® pad_token

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    TARGET_MODEL,
    quantization_config=quantization_config,
    device_map=device_map
)
```

#### åŠ è½½æ•°æ®é›†(æ•°æ®å‡†å¤‡)

```python
# åŠ è½½æ•°æ®é›†
DATASET_NAME = ''
dataset = load_dataset(DATASET_NAME, trust_remote_code=True)
```

å¦‚æœæ•°æ®é›†ä¸­æ²¡æœ‰éªŒè¯é›†ï¼Œéœ€è¦æ‰‹åŠ¨åˆ’åˆ†
```python
# æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯é›†ï¼Œè‹¥æ²¡æœ‰åˆ™æ‰‹åŠ¨åˆ’åˆ†
if 'validation' not in dataset:
    # è¿™æ˜¯ datasets åº“ä¸­çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
    # test_size=0.1ï¼šè¡¨ç¤ºå°†æ•°æ®é›†çš„ 10% åˆ†é…ç»™æµ‹è¯•é›†ï¼ˆtestï¼‰ï¼Œå‰©ä¸‹çš„ 90% ä½œä¸ºè®­ç»ƒé›†ï¼ˆtrainï¼‰
    # shuffle = Trueï¼š   è¡¨ç¤ºåœ¨åˆ’åˆ†ä¹‹å‰å¯¹æ•°æ®è¿›è¡Œéšæœºæ‰“ä¹±ï¼Œä»¥ç¡®ä¿æ•°æ®åˆ†å¸ƒçš„éšæœºæ€§ã€‚
    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç æ—¶åˆ’åˆ†ç»“æœçš„ä¸€è‡´æ€§ã€‚
    split_dataset  = dataset["train"].train_test_split(test_size=0.1, shuffle=True, seed=42)
    dataset["train"] = split_dataset ["train"]
    # å°†é‡æ–°åˆ’åˆ†åçš„æµ‹è¯•é›† dataset["test"] é‡å‘½åä¸ºéªŒè¯é›† dataset["validation"]ï¼Œä»¥ä¾¿åœ¨åç»­ä»£ç ä¸­æ˜ç¡®åŒºåˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç”¨é€”ã€‚
    dataset["validation"] = split_dataset ["test"]
```

#### é…ç½®Lora(ç¡®è®¤å¾®è°ƒæ–¹æ³•)

è‡³å…³é‡è¦çš„ä¸€æ­¥
```python
# åˆ›å»ºLoRAé…ç½®
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,  # è®­ç»ƒæ¨¡å¼
    r=8,  # Lora ç§©
    lora_alpha=32,  # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†
    lora_dropout=0.1,  # Dropout æ¯”ä¾‹
)
 
# å°†LoRAåº”ç”¨äºæ¨¡å‹
model = get_peft_model(model, config)
 
```

#### è®­ç»ƒé…ç½®
```python
# åˆ›å»ºå¾®è°ƒå‚æ•°
args = TrainingArguments(
    output_dir=os.path.join(BASE_DIR, 'output', 'Qwen2-0.5B'),
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    num_train_epochs=2,
    save_steps=100,
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True,
    report_to="none",
)

# ç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡ï¼Œè®­ç»ƒç»“æŸå
plot_callback = PlotLossCallback()
# åˆ›å»ºè®­ç»ƒå™¨æ¢°
trainer = Trainer(
    model=model,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    peft_config=config,
    tokenizer=tokenizer,
    args=training_args,
    data_collator=data_collator,
    callbacks=[plot_callback]
)
# å¼€å§‹è®­ç»ƒ
trainer.train()
# ä¿å­˜ LoRA æƒé‡
model.save_pretrained("./fine_tuned_model")

```


