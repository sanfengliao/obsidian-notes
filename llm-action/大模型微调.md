å¾®è°ƒï¼ˆFine-Tunineï¼‰æ˜¯åœ¨é¢„è®­ç»ƒå¤§æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–åœºæ™¯è¿›è¡Œå°èŒƒå›´çš„è°ƒæ•´ä¼˜åŒ–ã€‚å®ƒçš„ç›®æ ‡æ˜¯åœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªå¤§æ¨¡å‹çš„å‰æä¸‹ï¼Œé€šè¿‡è°ƒæ•´éƒ¨åˆ†å‚æ•°æˆ–ç‰¹å®šæ¨¡å—ï¼Œå®ç°æ¨¡å‹å¯¹å…·ä½“ä»»åŠ¡çš„å¿«é€Ÿé€‚é…ã€‚
[å¤§æ¨¡å‹ç›¸å…³æŠ€æœ¯åŸç†ä»¥åŠå®æˆ˜ç»éªŒï¼ˆå¤§æ¨¡å‹å·¥ç¨‹åŒ–ã€å¤§æ¨¡å‹åº”ç”¨è½åœ°ï¼‰](https://github.com/liguodongiot/llm-action/tree/main)
# å…¨å‚æ•°å¾®è°ƒ
è°ƒæ•´æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œé€‚ç”¨äºæ•°æ®é‡å¤§çš„æƒ…å†µï¼Œä½†è®¡ç®—èµ„æºæ¶ˆè€—é«˜ã€‚ç”±äºå¤§è¯­è¨€æ¨¡å‹å‚æ•°è§„æ¨¡å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œè¿™ä½¿å¾—åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿›è¡Œå…¨é‡å¾®è°ƒå˜å¾—ä¸å¯è¡Œã€‚

# å‚æ•°é«˜æ•ˆå¾®è°ƒ

å¾®è°ƒå°‘é‡æˆ–é¢å¤–çš„æ¨¡å‹å‚æ•°ï¼Œå›ºå®šå¤§éƒ¨åˆ†é¢„è®­ç»ƒæ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ï¼ŒåŒæ—¶ï¼Œä¹Ÿèƒ½å®ç°ä¸å…¨é‡å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æ¯”å…¨é‡å¾®è°ƒæ•ˆæœæ›´å¥½ï¼Œå¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°åŸŸå¤–åœºæ™¯ã€‚
é«˜æ•ˆå¾®è°ƒæŠ€æœ¯å¯ä»¥ç²—ç•¥åˆ†ä¸ºä»¥ä¸‹ä¸‰å¤§ç±»ï¼šå¢åŠ é¢å¤–å‚æ•°ï¼ˆAï¼‰ã€é€‰å–ä¸€éƒ¨åˆ†å‚æ•°æ›´æ–°ï¼ˆSï¼‰ã€å¼•å…¥é‡å‚æ•°åŒ–ï¼ˆRï¼‰ã€‚è€Œåœ¨å¢åŠ é¢å¤–å‚æ•°è¿™ç±»æ–¹æ³•ä¸­ï¼Œåˆä¸»è¦åˆ†ä¸ºç±»é€‚é…å™¨ï¼ˆAdapter-likeï¼‰æ–¹æ³•å’Œè½¯æç¤ºï¼ˆSoft promptsï¼‰ä¸¤ä¸ªå°ç±»ã€‚
![alt text](assets/1740382924772.png)

å¸¸è§çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯æœ‰BitFitã€Prefix Tuningã€Prompt Tuningã€P-Tuningã€Adapter Tuningã€LoRAç­‰ï¼Œåç»­æ–‡ç« å°†å¯¹ä¸€äº›ä¸»æµçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•è¿›è¡Œè®²è§£

ä»¥ä¸‹æ˜¯å‡ ç§ä¸»æµçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuning, PEFTï¼‰æ–¹æ³•çš„ç®€è¦ä»‹ç»ï¼š
## å¸¸è§å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸€ï¼‰-èƒŒæ™¯ã€å‚æ•°é«˜æ•ˆå¾®è°ƒç®€ä»‹](https://zhuanlan.zhihu.com/p/635152813)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆäºŒï¼‰-BitFitã€Prefix Tuningã€Prompt Tuning](https://zhuanlan.zhihu.com/p/635686756)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸‰ï¼‰-P-Tuningã€P-Tuning v2](https://zhuanlan.zhihu.com/p/635848732)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆå››ï¼‰-Adapter TuningåŠå…¶å˜ä½“](https://zhuanlan.zhihu.com/p/636038478)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆäº”ï¼‰-LoRAã€AdaLoRAã€QLoRA](https://zhuanlan.zhihu.com/p/636215898)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆå…­ï¼‰-MAM Adapterã€UniPELT](https://zhuanlan.zhihu.com/p/636362246)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸ƒï¼‰-æœ€ä½³å®è·µã€æ€»ç»“](https://zhuanlan.zhihu.com/p/649755252)
### **1. BitFitï¼ˆBias-term Fine-Tuningï¼‰**
- **åŸç†**ï¼šä»…å¾®è°ƒæ¨¡å‹ä¸­çš„**åç½®å‚æ•°**ï¼ˆbias termsï¼‰ï¼Œå†»ç»“å…¶ä»–æ‰€æœ‰æƒé‡å‚æ•°ã€‚
- **ç‰¹ç‚¹**ï¼š
  - è®¡ç®—æˆæœ¬æä½ï¼Œä»…éœ€è®­ç»ƒçº¦0.1%çš„å‚æ•°é‡ã€‚
  - é€‚ç”¨äºç®€å•ä»»åŠ¡æˆ–èµ„æºå—é™åœºæ™¯ã€‚
  - åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸­è¡¨ç°æ¥è¿‘å…¨é‡å¾®è°ƒã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šè½»é‡çº§é€‚é…ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€‚
### **2. Prefix Tuning**
- **åŸç†**ï¼šåœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ **å¯è®­ç»ƒçš„å‰ç¼€å‘é‡**ï¼ˆprefix tokensï¼‰ï¼Œé€šè¿‡è°ƒæ•´è¿™äº›å‘é‡å¼•å¯¼æ¨¡å‹è¾“å‡ºã€‚
  - å‰ç¼€å‘é‡æ˜¯è¿ç»­çš„ï¼ˆéçœŸå®tokenï¼‰ï¼Œé€šè¿‡æ¢¯åº¦ä¼˜åŒ–å­¦ä¹ ã€‚
  - é€šå¸¸åº”ç”¨äºTransformerçš„æ¯ä¸€å±‚ã€‚
- **ç‰¹ç‚¹**ï¼š
  - å‚æ•°é«˜æ•ˆï¼ˆä»…è°ƒæ•´çº¦0.1%-1%çš„å‚æ•°é‡ï¼‰ã€‚
  - é€‚åˆç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚å¯¹è¯ã€æ‘˜è¦ï¼‰ã€‚
- **æ”¹è¿›**ï¼šä¸ç¦»æ•£çš„Promptä¸åŒï¼ŒPrefixæ˜¯è¿ç»­å¯å­¦ä¹ çš„å‘é‡ï¼Œä¼˜åŒ–æ›´ç¨³å®šã€‚
### **3. Prompt Tuning**
- **åŸç†**ï¼šåœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ **å¯å­¦ä¹ çš„æç¤ºå‘é‡**ï¼ˆsoft promptsï¼‰ï¼Œé€šè¿‡è°ƒæ•´è¿™äº›å‘é‡é€‚é…ä»»åŠ¡ã€‚
  - ä»…ä½œç”¨äºè¾“å…¥å±‚ï¼Œä¸æ¶‰åŠæ¨¡å‹å†…éƒ¨ç»“æ„ã€‚
  - æç¤ºå‘é‡çš„é•¿åº¦å’Œå‚æ•°å¯è°ƒã€‚
- **ç‰¹ç‚¹**ï¼š
  - å‚æ•°é‡æä½ï¼ˆé€šå¸¸ä¸º0.01%-0.1%ï¼‰ã€‚
  - å¯¹æ¨¡å‹è§„æ¨¡æ•æ„Ÿï¼Œå¤§æ¨¡å‹ï¼ˆå¦‚ç™¾äº¿å‚æ•°ï¼‰æ•ˆæœæ›´ä½³ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šæ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ä»»åŠ¡ã€‚

### **4. P-Tuningï¼ˆPrompt Tuningçš„å‡çº§ç‰ˆï¼‰**
- **åŸç†**ï¼šå¼•å…¥**è¿ç»­å¯å­¦ä¹ çš„æç¤ºå‘é‡**ï¼Œå¹¶é€šè¿‡å°å‹ç¥ç»ç½‘ç»œï¼ˆå¦‚LSTMæˆ–MLPï¼‰ç”Ÿæˆæ›´å¤æ‚çš„æç¤ºã€‚
  - è§£å†³ä¼ ç»Ÿç¦»æ•£æç¤ºï¼ˆhard promptï¼‰éš¾ä»¥ä¼˜åŒ–çš„é—®é¢˜ã€‚
- **ç‰¹ç‚¹**ï¼š
  - ä¼˜åŒ–æ›´ç¨³å®šï¼Œæ”¯æŒå¤æ‚ä»»åŠ¡ï¼ˆå¦‚çŸ¥è¯†æ¨ç†ï¼‰ã€‚
  - ç›¸æ¯”æ™®é€šPrompt Tuningï¼Œæ•ˆæœæ›´ä¼˜ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å¤æ‚æç¤ºç»“æ„çš„ä»»åŠ¡ï¼ˆå¦‚é—®ç­”ã€é€»è¾‘æ¨ç†ï¼‰ã€‚

### **5. Adapter Tuning**
- **åŸç†**ï¼šåœ¨Transformerå±‚çš„**å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰åæ’å…¥å°å‹é€‚é…å™¨æ¨¡å—**ï¼ˆAdapterï¼‰ã€‚
  - Adapteré€šå¸¸åŒ…å«é™ç»´ï¼ˆdown-projectionï¼‰å’Œå‡ç»´ï¼ˆup-projectionï¼‰çš„å…¨è¿æ¥å±‚ã€‚
  - ä»…è®­ç»ƒAdapterå‚æ•°ï¼Œå†»ç»“åŸæ¨¡å‹å‚æ•°ã€‚
- **ç‰¹ç‚¹**ï¼š
  - é€‚é…å™¨å‚æ•°é‡å°ï¼ˆçº¦1%-5%ï¼‰ï¼Œé€‚åˆé€å±‚é€‚é…ã€‚
  - å¯èƒ½ç•¥å¾®å¢åŠ æ¨ç†å»¶è¿Ÿï¼ˆéœ€ä¸²è¡Œè®¡ç®—é€‚é…å™¨ï¼‰ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šå¤šä»»åŠ¡å­¦ä¹ ã€å¤šè¯­è¨€é€‚é…ã€‚

### **6. LoRAï¼ˆLow-Rank Adaptationï¼‰**
- **åŸç†**ï¼šé€šè¿‡**ä½ç§©åˆ†è§£**è¿‘ä¼¼æƒé‡æ›´æ–°ï¼Œåœ¨åŸå§‹æƒé‡çŸ©é˜µæ—æ·»åŠ ä½ç§©çŸ©é˜µï¼ˆå¦‚Î”W = AÂ·Bï¼ŒAå’ŒBä¸ºä½ç§©çŸ©é˜µï¼‰ã€‚
  - ä»…è®­ç»ƒä½ç§©çŸ©é˜µå‚æ•°ï¼Œå†»ç»“åŸæ¨¡å‹æƒé‡ã€‚
  - è®­ç»ƒåå°†ä½ç§©çŸ©é˜µåˆå¹¶å›åŸæ¨¡å‹ï¼Œ**æ¨ç†æ—¶æ— é¢å¤–è®¡ç®—**ã€‚
- **ç‰¹ç‚¹**ï¼š
  - å‚æ•°é‡å°ï¼ˆé€šå¸¸ä¸º0.5%-2%ï¼‰ï¼Œè®¡ç®—é«˜æ•ˆã€‚
  - å‡ ä¹ä¸å¢åŠ æ¨ç†å»¶è¿Ÿï¼Œé€‚åˆå¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚GPT-3ã€LLaMAï¼‰ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šç”Ÿæˆä»»åŠ¡ã€å¤§è§„æ¨¡æ¨¡å‹å¾®è°ƒã€‚


| æ–¹æ³•                | æ ¸å¿ƒæ€æƒ³         | å‚æ•°é‡å æ¯”      | è®¡ç®—æˆæœ¬ | é€‚ç”¨åœºæ™¯      | ä¼˜ç‚¹         | ç¼ºç‚¹       |
| ----------------- | ------------ | ---------- | ---- | --------- | ---------- | -------- |
| **BitFit**        | ä»…å¾®è°ƒåç½®å‚æ•°      | ~0.1%      | æä½   | ç®€å•åˆ†ç±»ä»»åŠ¡    | æç®€å®ç°ï¼Œèµ„æºå‹å¥½  | ä»»åŠ¡é€‚é…èƒ½åŠ›æœ‰é™ |
| **Prefix Tuning** | æ·»åŠ å¯å­¦ä¹ å‰ç¼€å‘é‡    | 0.1%-1%    | ä¸­ç­‰   | ç”Ÿæˆä»»åŠ¡      | æ•ˆæœæ¥è¿‘å…¨é‡å¾®è°ƒ   | ä¼˜åŒ–éš¾åº¦è¾ƒé«˜   |
| **Prompt Tuning** | å­¦ä¹ è¾“å…¥å±‚æç¤ºå‘é‡    | 0.01%-0.1% | ä½    | ç”Ÿæˆ/åˆ†ç±»ä»»åŠ¡   | å‚æ•°æå°‘ï¼Œé€‚åˆå¤§æ¨¡å‹ | ä¾èµ–æ¨¡å‹è§„æ¨¡   |
| **P-Tuning**      | é€šè¿‡ç¥ç»ç½‘ç»œç”Ÿæˆè¿ç»­æç¤º | 0.1%-1%    | ä¸­ç­‰   | å¤æ‚æ¨ç†ä»»åŠ¡    | æç¤ºä¼˜åŒ–æ›´çµæ´»    | å®ç°å¤æ‚åº¦è¾ƒé«˜  |
| **Adapter**       | æ’å…¥å°å‹é€‚é…å™¨æ¨¡å—    | 1%-5%      | ä¸­ç­‰   | å¤šä»»åŠ¡/å¤šè¯­è¨€åœºæ™¯ | æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±• | å¯èƒ½å¢åŠ æ¨ç†å»¶è¿Ÿ |
| **LoRA**          | ä½ç§©åˆ†è§£è¿‘ä¼¼æƒé‡æ›´æ–°   | 0.5%-2%    | ä½    | å¤§è§„æ¨¡æ¨¡å‹å¾®è°ƒ   | é«˜æ•ˆä¸”æ— æ¨ç†å¼€é”€   | éœ€è®¾è®¡ä½ç§©ç»“æ„  |
|                   |              |            |      |           |            |          |
## å¾®è°ƒæµç¨‹

```mermaid
graph LR
    A[æ˜ç¡®ä»»åŠ¡ç›®æ ‡] --> B[æ•°æ®å‡†å¤‡]
    B --> C[é€‰æ‹©åŸºåº§æ¨¡å‹]
    C --> D[ç¡®å®šå¾®è°ƒæ–¹æ³•]
    D --> E[è®­ç»ƒé…ç½®]
    E --> F[æ¨¡å‹è¯„ä¼°]
    F --> G[éƒ¨ç½²ä¼˜åŒ–]
```



# å¸¸è§çš„å¾®è°ƒæ¡†æ¶
## Hugging Face
Hugging Face ç”Ÿæ€æ˜¯ä¸€ä¸ªè¦†ç›– NLPã€CVã€è¯­éŸ³ç­‰å¤šé¢†åŸŸçš„å¼€æºå·¥å…·é“¾ï¼Œæä¾›ä»æ•°æ®ç®¡ç†ã€æ¨¡å‹è®­ç»ƒã€å¾®è°ƒã€è¯„ä¼°åˆ°éƒ¨ç½²çš„å…¨æµç¨‹æ”¯æŒã€‚**å®˜ç½‘**ï¼š https://huggingface.co/docs
### ä¸»è¦ç»„ä»¶
####  Transformers åº“
- **å®šä½**ï¼šNLP/CV/è¯­éŸ³æ¨¡å‹çš„**æ ¸å¿ƒåº“**ï¼Œæ”¯æŒé¢„è®­ç»ƒæ¨¡å‹åŠ è½½ã€å¾®è°ƒä¸æ¨ç†ã€‚  
- **æ ¸å¿ƒåŠŸèƒ½**ï¼š  
  - **æ¨¡å‹æ”¯æŒ**ï¼šè¶… 20 ä¸‡å…¬å¼€æ¨¡å‹ï¼ˆBERTã€GPTã€ViTã€Whisper ç­‰ï¼‰ï¼Œæ”¯æŒ PyTorchã€TensorFlowã€JAX æ¡†æ¶ã€‚  
  - **ç»Ÿä¸€æ¥å£**ï¼šé€šè¿‡ `AutoModel`ã€`AutoTokenizer` è‡ªåŠ¨åŒ¹é…æ¨¡å‹æ¶æ„ã€‚  
  ```python
  from transformers import AutoModelForSequenceClassification, AutoTokenizer
  model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
  tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
  ```
  - **è®­ç»ƒå·¥å…·**ï¼š`Trainer` ç±»å°è£…è®­ç»ƒæµç¨‹ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ç­‰ã€‚  
- **é€‚ç”¨åœºæ™¯**ï¼šæ–‡æœ¬åˆ†ç±»ã€ç”Ÿæˆã€ç¿»è¯‘ã€å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ã€‚
#### Datasets åº“
- **å®šä½**ï¼šé«˜æ•ˆç®¡ç†ã€é¢„å¤„ç†å’Œå…±äº«æ•°æ®é›†ã€‚  
- **æ ¸å¿ƒåŠŸèƒ½**ï¼š  
  - **æ•°æ®é›†ä»“åº“**ï¼šæä¾› 3 ä¸‡+å…¬å¼€æ•°æ®é›†ï¼ˆå¦‚ GLUEã€SQuADã€COCOï¼‰ã€‚  
  - **æµå¼åŠ è½½**ï¼šæ”¯æŒè¶…å¤§æ•°æ®é›†ï¼ˆå¦‚ Common Crawlï¼‰çš„ç£ç›˜æ˜ å°„åŠ è½½ï¼Œé¿å…å†…å­˜æº¢å‡ºã€‚  
  ```python
  from datasets import load_dataset
  dataset = load_dataset("glue", "mrpc", split="train", streaming=True)  # æµå¼åŠ è½½
  ```
  - **æ•°æ®é¢„å¤„ç†**ï¼šä¸ Tokenizers åº“æ— ç¼é›†æˆï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†ã€‚  
- **ä¼˜åŠ¿**ï¼šæ ‡å‡†åŒ–æ•°æ®æ ¼å¼ï¼ˆArrowï¼‰ï¼Œå…¼å®¹ Pandas/Numpyã€‚

#### PEFTï¼ˆParameter-Efficient Fine-Tuning
- **å®šä½**ï¼š**å‚æ•°é«˜æ•ˆå¾®è°ƒ**å·¥å…·ï¼Œé™ä½å¤§æ¨¡å‹è®­ç»ƒæˆæœ¬ã€‚  
- **æ”¯æŒæ–¹æ³•**ï¼šLoRAã€Prefix Tuningã€Adaptersã€Prompt Tuning ç­‰ã€‚  
- **ç¤ºä¾‹**ï¼ˆLoRA å¾®è°ƒï¼‰ï¼š  
  ```python
  from peft import LoraConfig, get_peft_model
  config = LoraConfig(r=8, target_modules=["query", "value"])
  model = get_peft_model(model, config)  # åŸå§‹æ¨¡å‹çš„ 0.1% å‚æ•°è¢«è®­ç»ƒ
  ```
- **ä¼˜åŠ¿**ï¼šæ˜¾å­˜å ç”¨å‡å°‘ 50%~90%ï¼Œé€‚åˆä½èµ„æºåœºæ™¯ã€‚

#### Tokenizers åº“
- **å®šä½**ï¼šé«˜æ€§èƒ½æ–‡æœ¬åˆ†è¯å·¥å…·ï¼Œæ”¯æŒ 100+è¯­è¨€ã€‚  
- **æ ¸å¿ƒåŠŸèƒ½**ï¼š  
  - **é¢„ç½®åˆ†è¯å™¨**ï¼šBPEã€WordPieceã€SentencePiece ç­‰ç®—æ³•ã€‚  
  - **è‡ªå®šä¹‰åˆ†è¯å™¨**ï¼šä»é›¶è®­ç»ƒæˆ–æ‰©å±•ç°æœ‰åˆ†è¯å™¨ã€‚  
  ```python
  from tokenizers import BertWordPieceTokenizer
  tokenizer = BertWordPieceTokenizer()
  tokenizer.train(files=["text.txt"], vocab_size=30522)
  tokenizer.save("custom-tokenizer.json")
  ```
- **ä¼˜åŠ¿**ï¼šRust åç«¯åŠ é€Ÿï¼Œæ¯”åŸç”Ÿ Python å¿« 10 å€ã€‚


#### Hugging Face Hub
- **å®šä½**ï¼šæ¨¡å‹ã€æ•°æ®é›†å’Œåº”ç”¨çš„**åä½œå¹³å°**ï¼ˆç±»ä¼¼ GitHub for MLï¼‰ã€‚  
- **æ ¸å¿ƒåŠŸèƒ½**ï¼š  
  - **æ¨¡å‹æ‰˜ç®¡**ï¼šç”¨æˆ·å¯ä¸Šä¼ /ä¸‹è½½æ¨¡å‹ï¼ˆå«ç‰ˆæœ¬æ§åˆ¶ï¼‰ã€‚  
  - **æ•°æ®é›†æ‰˜ç®¡**ï¼šæ”¯æŒå…¬å¼€æˆ–ç§æœ‰æ•°æ®é›†å­˜å‚¨ã€‚  
  - **Spaces**ï¼šä¸€é”®éƒ¨ç½² ML åº”ç”¨ï¼ˆå¦‚ Gradio/Streamlit åº”ç”¨ï¼‰ã€‚  
  - **ç¤¾åŒºäº’åŠ¨**ï¼šæ¨¡å‹è¯„æµ‹ã€è®¨è®ºã€åˆ†å‰ï¼ˆForkï¼‰åŠŸèƒ½ã€‚  
- **ç¤ºä¾‹**ï¼š  
  ```python
  # ä¸Šä¼ æ¨¡å‹åˆ° Hub
  model.push_to_hub("my-bert-finetuned")
  # åŠ è½½ç¤¾åŒºæ¨¡å‹
  model = AutoModel.from_pretrained("username/my-bert-finetuned")
  ```



#### Accelerate åº“
- **å®šä½**ï¼š**ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒ**ï¼Œæ”¯æŒå¤š GPU/TPU è®­ç»ƒã€‚  
- **æ ¸å¿ƒåŠŸèƒ½**ï¼š  
  - **ç»Ÿä¸€è®­ç»ƒæ¥å£**ï¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†å‘ã€æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€‚  
  - **å…¼å®¹æ€§**ï¼šæ”¯æŒ PyTorchã€Transformersã€è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚  
  ```python
  from accelerate import Accelerator
  accelerator = Accelerator()
  model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)
  ```
- **é€‚ç”¨åœºæ™¯**ï¼šå•æœºå¤šå¡æˆ–å¤šæœºè®­ç»ƒï¼Œæ— éœ€ä¿®æ”¹å¤§é‡ä»£ç ã€‚



#### Gradio
- **å®šä½**ï¼šå¿«é€Ÿæ„å»º ML æ¨¡å‹**äº¤äº’å¼æ¼”ç¤ºç•Œé¢**ã€‚  
- **æ ¸å¿ƒåŠŸèƒ½**ï¼š  
  - **é¢„ç½®ç»„ä»¶**ï¼šæ–‡æœ¬æ¡†ã€å›¾åƒä¸Šä¼ ã€å®æ—¶å¯è§†åŒ–ç­‰ã€‚  
  - **ä¸€é”®éƒ¨ç½²**ï¼šå¯æ‰˜ç®¡åˆ° Hugging Face Spacesã€‚  
  ```python
  import gradio as gr
  def classify(text):
      return model(text)  # è°ƒç”¨æ¨¡å‹æ¨ç†
  gr.Interface(fn=classify, inputs="text", outputs="label").launch()
  ```
- **é€‚ç”¨åœºæ™¯**ï¼šæ¨¡å‹æ¼”ç¤ºã€ç”¨æˆ·æµ‹è¯•ã€åŸå‹éªŒè¯ã€‚

#### Optimum
- **å®šä½**ï¼š**ç¡¬ä»¶åŠ é€Ÿæ¨ç†**å·¥å…·ï¼Œæ”¯æŒ ONNXã€TensorRTã€OpenVINO ç­‰ã€‚  
- **æ ¸å¿ƒåŠŸèƒ½**ï¼š  
  - **æ¨¡å‹å¯¼å‡º**ï¼šå°† Transformers æ¨¡å‹è½¬æ¢ä¸ºä¼˜åŒ–æ ¼å¼ã€‚  
  - **åŠ é€Ÿæ¨ç†**ï¼šä¸ç¡¬ä»¶å‚å•†ï¼ˆIntelã€NVIDIAï¼‰æ·±åº¦é›†æˆã€‚  
  ```python
  from optimum.onnxruntime import ORTModelForSequenceClassification
  model = ORTModelForSequenceClassification.from_pretrained("bert-base-uncased")
  ```
- **ä¼˜åŠ¿**ï¼šæ¨ç†é€Ÿåº¦æå‡ 2-5 å€ï¼Œå»¶è¿Ÿé™ä½ã€‚

#### Autotrain
- **å®šä½**ï¼š**è‡ªåŠ¨åŒ–æ¨¡å‹è®­ç»ƒ**ï¼Œæ— éœ€ç¼–å†™ä»£ç ã€‚  
- **æ”¯æŒä»»åŠ¡**ï¼šæ–‡æœ¬åˆ†ç±»ã€ç”Ÿæˆã€å›¾åƒåˆ†ç±»ã€è¡¨æ ¼æ•°æ®ç­‰ã€‚  
- **ä½¿ç”¨æ–¹å¼**ï¼š  
  ```bash
  # å‘½ä»¤è¡Œå¯åŠ¨è®­ç»ƒ
  autotrain llm --model meta-llama/Llama-3-8b --data_path my_data.json
  ```
- **é€‚ç”¨åœºæ™¯**ï¼šå¿«é€ŸåŸºçº¿æ¨¡å‹è®­ç»ƒï¼ŒéæŠ€æœ¯ç”¨æˆ·å‹å¥½ã€‚

#### å…¶ä»–å·¥å…·
- **Model Cards**ï¼šæ ‡å‡†åŒ–æ¨¡å‹æ–‡æ¡£æ¨¡æ¿ï¼Œè®°å½•æ¨¡å‹ç”¨é€”ã€åå·®ã€è®­ç»ƒæ•°æ®ç­‰ã€‚  
- **Evaluate**ï¼šç»Ÿä¸€è¯„ä¼°æŒ‡æ ‡åº“ï¼ˆå¦‚å‡†ç¡®ç‡ã€BLEUã€ROUGEï¼‰ã€‚  
- **Diffusers**ï¼šæ‰©æ•£æ¨¡å‹ï¼ˆStable Diffusionï¼‰è®­ç»ƒä¸æ¨ç†åº“ã€‚  
- **TRL**ï¼ˆTransformer Reinforcement Learningï¼‰ï¼šæ”¯æŒ RLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰ã€‚  

### **ç”Ÿæ€åä½œæµç¨‹å›¾**  
```mermaid
graph LR
  A[DatasetsåŠ è½½æ•°æ®] --> B[Tokenizersåˆ†è¯]
  B --> C[Transformersè®­ç»ƒ/å¾®è°ƒ]
  C --> D[Hubæ‰˜ç®¡æ¨¡å‹]
  D --> E[Gradioæ„å»ºç•Œé¢]
  E --> F[Spaceséƒ¨ç½²åº”ç”¨]
  C --ä½èµ„æºåœºæ™¯--> G[PEFTå¾®è°ƒ]
  C --åˆ†å¸ƒå¼è®­ç»ƒ--> H[Accelerate]
  C --ç¡¬ä»¶åŠ é€Ÿ--> I[Optimum]
```


### **ä¼˜åŠ¿æ€»ç»“**  
1. **å…¨æµç¨‹è¦†ç›–**ï¼šä»æ•°æ®åˆ°éƒ¨ç½²ï¼Œå·¥å…·é“¾æ— ç¼è¡”æ¥ã€‚  
2. **ç¤¾åŒºé©±åŠ¨**ï¼šå¼€æºåä½œï¼Œæ¨¡å‹ã€æ•°æ®é›†å…±äº«ä¾¿æ·ã€‚  
3. **çµæ´»æ€§**ï¼šæ”¯æŒä»ä»£ç çº§å®šåˆ¶åˆ°æ— ä»£ç è‡ªåŠ¨åŒ–ï¼ˆAutotrainï¼‰ã€‚  
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šTokenizersï¼ˆRustï¼‰ã€Optimumï¼ˆç¡¬ä»¶åŠ é€Ÿï¼‰ç­‰åº•å±‚ä¼˜åŒ–ã€‚  


### **ä½¿ç”¨å»ºè®®**  
- **å¿«é€ŸåŸå‹å¼€å‘**ï¼šTransformers + Datasets + Gradioã€‚  
- **ä½èµ„æºå¾®è°ƒ**ï¼šPEFT + Accelerateã€‚  
- **å·¥ä¸šçº§éƒ¨ç½²**ï¼šOptimum + ONNX/TensorRTã€‚  
- **åä½œä¸åˆ†äº«**ï¼šHub + Spacesã€‚  


### ç¤ºä¾‹
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")


tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")


from peft import LoraConfig, get_peft_model

# è®¾ç½®å¾®è°ƒæ–¹æ³•
lora_config = LoraConfig(
    r=8,  # ä½ç§©çŸ©é˜µçš„ç§©
    target_modules=["q_proj", "v_proj"],  # ç›®æ ‡æ¨¡å—ï¼ˆå¦‚LLaMAçš„æ³¨æ„åŠ›å±‚ï¼‰
)
model = get_peft_model(model, lora_config)

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="output",
    per_device_train_batch_size=4,
    fp16=True,  # æ··åˆç²¾åº¦è®­ç»ƒ
    gradient_accumulation_steps=2,
)

# è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()
```

## DeepSeed
DeepSpeed æ˜¯ç”± Microsoft å¼€å‘çš„å¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œä¸“æ³¨äº â€‹**å¤§è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒä¸æ¨ç†**ã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºè§£å†³åƒäº¿çº§å‚æ•°æ¨¡å‹çš„æ˜¾å­˜å ç”¨ã€è®¡ç®—æ•ˆç‡å’Œåˆ†å¸ƒå¼æ‰©å±•é—®é¢˜ã€‚ä¸Hugging Faceç”Ÿæ€çš„æ·±åº¦æ•´åˆï¼ˆå¦‚é€šè¿‡`Trainer`ç›´æ¥è°ƒç”¨ï¼‰ï¼Œä½¿å¾—å¼€å‘è€…èƒ½åœ¨æ˜“ç”¨æ€§ä¸æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ **å®˜ç½‘** https://www.deepspeed.ai/
### æ ¸å¿ƒä¼˜åŠ¿
#### æ˜¾å­˜ä¼˜åŒ–èƒ½åŠ›ï¼ˆZeRO æŠ€æœ¯)
- **æ”¯æŒåƒäº¿çº§æ¨¡å‹è®­ç»ƒ**ï¼š  
  - **ZeRO-Offload**ï¼šå°†ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦ã€å‚æ•°å¸è½½åˆ° CPU æˆ– NVMeï¼Œæ˜¾å­˜å ç”¨é™ä½ **10 å€ä»¥ä¸Š**ã€‚  
  - **ZeRO-Infinity**ï¼šæ”¯æŒä¸‡äº¿å‚æ•°æ¨¡å‹è®­ç»ƒï¼ˆå¦‚ [BLOOM-176B](https://huggingface.co/bigscience/bloom)ï¼‰ã€‚  
- **å¯¹æ¯” Hugging Face åŸç”Ÿè®­ç»ƒ**ï¼š  
  - Hugging Face `Trainer` ä¾èµ– PyTorch åŸç”Ÿæ˜¾å­˜ç®¡ç†ï¼Œå…¨é‡å¾®è°ƒ 70B æ¨¡å‹éœ€æ•°ç™¾ GB æ˜¾å­˜ï¼Œè€Œ DeepSpeed ZeRO-3 + Offload å¯åœ¨å•æœº 8*A100ï¼ˆ80GBï¼‰ ä¸Šå®Œæˆã€‚

#### åˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡
- **3D å¹¶è¡Œç­–ç•¥**ï¼š  
  - **æ•°æ®å¹¶è¡Œ** + **æ¨¡å‹å¹¶è¡Œ**ï¼ˆå¼ é‡/æµæ°´çº¿å¹¶è¡Œï¼‰çš„æ··åˆç­–ç•¥ï¼Œæ”¯æŒè¶…å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒã€‚  
  - Hugging Face ä»…æ”¯æŒåŸºç¡€æ•°æ®å¹¶è¡Œï¼ˆéœ€ç»“åˆ `accelerate` åº“æ‰©å±•ï¼‰ã€‚  
- **é€šä¿¡ä¼˜åŒ–**ï¼š  
  - **æ¢¯åº¦å‹ç¼©**ï¼ˆ1-bit Adamï¼‰å‡å°‘é€šä¿¡é‡ï¼Œ**å¼‚æ­¥æµæ°´çº¿**å‡å°‘ GPU ç©ºé—²æ—¶é—´ã€‚  
  - åœ¨ 100Gbps InfiniBand é›†ç¾¤ä¸­ï¼ŒDeepSpeed ååé‡æ¯”åŸç”Ÿ PyTorch é«˜ **2-3 å€**ã€‚

#### ç¡¬ä»¶èµ„æºåˆ©ç”¨ç‡
- **æ··åˆç²¾åº¦ä¸é‡åŒ–**ï¼š  
  - æ”¯æŒ FP16/BF16/INT8 è®­ç»ƒï¼Œç›¸æ¯” Hugging Face çš„ `fp16=True` é€‰é¡¹ï¼ŒDeepSpeed çš„æ··åˆç²¾åº¦å®ç°æ›´é«˜æ•ˆï¼ˆæ˜¾å­˜èŠ‚çœ + é€Ÿåº¦æå‡ï¼‰ã€‚  
- **CPU/NVMe å¸è½½**ï¼š  
  - å…è®¸å°†éƒ¨åˆ†è®¡ç®—å¸è½½åˆ° CPU æˆ– SSDï¼Œçªç ´ GPU æ˜¾å­˜é™åˆ¶ã€‚

#### å¤§è§„æ¨¡å…¨é‡å¾®è°ƒ
- **é€‚ç”¨åœºæ™¯**ï¼š  
  - DeepSpeed ä¸“ä¸º **å…¨é‡å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰** è®¾è®¡ï¼Œè€Œ Hugging Face æ›´ä¾èµ– PEFTï¼ˆå¦‚ LoRAï¼‰åº”å¯¹æ˜¾å­˜ç“¶é¢ˆã€‚  
  - å¯¹äºéœ€è¦æ›´æ–°å…¨éƒ¨å‚æ•°çš„åœºæ™¯ï¼ˆå¦‚é¢†åŸŸè‡ªé€‚åº”ï¼‰ï¼ŒDeepSpeed æ˜¯æ›´ä¼˜é€‰æ‹©ã€‚

### å’ŒHugging Faceå¯¹æ¯”
| **ç‰¹æ€§**    | **DeepSpeed**                | **Hugging Face Transformers** |
| --------- | ---------------------------- | ----------------------------- |
| **æ ¸å¿ƒç›®æ ‡**  | è¶…å¤§è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒ                  | å¿«é€Ÿæ¨¡å‹å®éªŒä¸è½»é‡çº§å¾®è°ƒ                  |
| **æ˜¾å­˜ä¼˜åŒ–**  | ZeRO åˆ†ç‰‡ + Offloadï¼ˆæ”¯æŒ 1T+ å‚æ•°ï¼‰ | ä¾èµ– PEFTï¼ˆLoRA ç­‰ï¼‰æˆ–åŸºç¡€æ··åˆç²¾åº¦        |
| **åˆ†å¸ƒå¼è®­ç»ƒ** | åŸç”Ÿæ”¯æŒ 3D å¹¶è¡Œï¼ˆæ•°æ®/æ¨¡å‹/æµæ°´çº¿ï¼‰        | éœ€ç»“åˆ `accelerate` å®ç°åŸºç¡€æ•°æ®å¹¶è¡Œ     |
| **ç¡¬ä»¶è¦æ±‚**  | å¤š GPU/CPU/NVMe æ‰©å±•            | å•å¡æˆ–å°‘é‡ GPU å³å¯è¿è¡Œ                |
| **é€‚ç”¨åœºæ™¯**  | å…¨é‡å¾®è°ƒ 10B+ æ¨¡å‹ã€å·¥ä¸šçº§è®­ç»ƒé›†ç¾¤         | 7B ä»¥ä¸‹æ¨¡å‹å¾®è°ƒã€å¿«é€ŸåŸå‹å¼€å‘              |
| **ä»£ç å¤æ‚åº¦** | é«˜ï¼ˆéœ€é…ç½® JSON æ–‡ä»¶å’Œåˆ†å¸ƒå¼å‚æ•°ï¼‰         | ä½ï¼ˆå‡ è¡Œä»£ç è°ƒç”¨ `Trainer`ï¼‰           |
| **å…¸å‹ç”¨æˆ·**  | ä¼ä¸šçº§å¤§æ¨¡å‹å›¢é˜Ÿã€è¶…ç®—ä¸­å¿ƒ                | ç ”ç©¶è€…ã€åˆåˆ›å…¬å¸ã€ä¸ªäººå¼€å‘è€…                |
|           |                              |                               |
å’ŒHugging Face å¹¶éäºŒé€‰ä¸€ï¼Œè€Œæ˜¯ â€‹**ååŒä½¿ç”¨**â€‹ ä»¥å…¼é¡¾æ˜“ç”¨æ€§ä¸æ€§èƒ½
```python

from transformers import Trainer, TrainingArguments

# å¯ç”¨ DeepSpeed é…ç½®
training_args = TrainingArguments(
    output_dir="output",
    deepspeed="ds_config.json",  # DeepSpeed é…ç½®æ–‡ä»¶
    per_device_train_batch_size=4,
    fp16=True,
)

# æ ‡å‡† Hugging Face è®­ç»ƒæµç¨‹
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()
```

## LLaMA Factory

**Llama Factory** æ˜¯ä¸€ä¸ªä¸“æ³¨äº **LLaMA ç³»åˆ—æ¨¡å‹**ï¼ˆåŒ…æ‹¬ LLaMAã€Alpacaã€Chinese-LLaMA ç­‰ï¼‰é«˜æ•ˆå¾®è°ƒçš„å¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–å¤§è¯­è¨€æ¨¡å‹åœ¨æŒ‡ä»¤è·Ÿéšã€é¢†åŸŸé€‚åº”ç­‰åœºæ™¯ä¸‹çš„å®šåˆ¶åŒ–è®­ç»ƒæµç¨‹ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯é™ä½å¤§æ¨¡å‹å¾®è°ƒçš„æŠ€æœ¯é—¨æ§›ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚

- **é¡¹ç›®åœ°å€**ï¼š[https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)  
- **æ ¸å¿ƒå¼€å‘è€…**ï¼šç¤¾åŒºé©±åŠ¨çš„å¼€æºé¡¹ç›®ï¼Œä¸»è¦ç»´æŠ¤è€…ä¸ºå›½å†…å¼€å‘è€…å›¢é˜Ÿã€‚  

---

### æ ¸å¿ƒåŠŸèƒ½
#### æ¨¡å‹æ”¯æŒ
- **å…¨ç³» LLaMA å…¼å®¹**ï¼šæ”¯æŒ LLaMA-1/2ã€Alpacaã€Vicunaã€Chinese-LLaMA ç­‰å˜ä½“ï¼Œ**å·²æ”¯æŒdeepseekã€‚**  
- **æ‰©å±•æ¨¡å‹**ï¼šå¯é€‚é…å…¶ä»–ç±» LLaMA æ¶æ„æ¨¡å‹ï¼ˆå¦‚ OpenLLMã€Baichuanã€Qwenï¼ŒDeepSeek ç­‰ï¼‰ã€‚  

#### å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰
- **ä¸»æµæ–¹æ³•é›†æˆ**ï¼š  
  - **LoRA**ï¼šä½ç§©çŸ©é˜µåˆ†è§£ï¼Œè®­ç»ƒå‚æ•°é‡å‡å°‘ 90% ä»¥ä¸Šã€‚  
  - **QLoRA**ï¼š4-bit é‡åŒ– + LoRAï¼Œå•å¡ï¼ˆ24GBï¼‰å¯å¾®è°ƒ 13B æ¨¡å‹ã€‚  
  - **Prefix Tuning**ï¼šå­¦ä¹ å¯è®­ç»ƒçš„å‰ç¼€å‘é‡ã€‚  
  - **Adapter**ï¼šæ’å…¥è½»é‡çº§é€‚é…æ¨¡å—ã€‚  
- **çµæ´»é…ç½®**ï¼šé€šè¿‡ YAML æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©å¾®è°ƒç­–ç•¥ã€‚  

#### è®­ç»ƒä¼˜åŒ–
- **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šæ”¯æŒ ZeROï¼ˆDeepSpeed é›†æˆï¼‰ã€æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€‚  
- **æ˜¾å­˜ä¼˜åŒ–**ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰ã€æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/BF16ï¼‰ã€‚  
- **ç¡¬ä»¶é€‚é…**ï¼šæ”¯æŒæ¶ˆè´¹çº§ GPUï¼ˆå¦‚ RTX 3090ï¼‰å’Œå¤šå¡é›†ç¾¤è®­ç»ƒã€‚  

#### æ•°æ®ä¸ä»»åŠ¡æ”¯æŒ
- **é¢„ç½®æ•°æ®é›†**ï¼š  
  - æŒ‡ä»¤æ•°æ®é›†ï¼ˆAlpaca-GPT4ã€ShareGPTï¼‰ã€å¯¹è¯æ•°æ®é›†ï¼ˆMultiWOZï¼‰ã€é¢†åŸŸæ•°æ®ï¼ˆåŒ»ç–—ã€æ³•å¾‹ï¼‰ã€‚  
  - æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ï¼ˆJSON/CSV æ ¼å¼ï¼‰ã€‚  
- **ä»»åŠ¡ç±»å‹**ï¼š  
  - æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆã€é—®ç­”ã€æ–‡æœ¬åˆ†ç±»ã€‚  

#### å·¥å…·é“¾é›†æˆ
- **Hugging Face ç”Ÿæ€**ï¼šæ— ç¼å¯¹æ¥ Transformersã€Datasetsã€PEFT ç­‰åº“ã€‚  
- **æ¨ç†éƒ¨ç½²**ï¼šå¯¼å‡º LoRA æƒé‡ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ï¼Œæ”¯æŒ ONNX/TensorRT åŠ é€Ÿã€‚  


### æŠ€æœ¯ä¼˜åŠ¿ 
####  é«˜æ•ˆæ€§ 
- **èµ„æºå ç”¨ä½**ï¼šQLoRA å¾®è°ƒ 7B æ¨¡å‹ä»…éœ€ 10GB æ˜¾å­˜ï¼ˆRTX 3080ï¼‰ã€‚  
- **è®­ç»ƒé€Ÿåº¦å¿«**ï¼šé€šè¿‡ DeepSpeed ZeRO-2 ä¼˜åŒ–ï¼Œååé‡æ¯”åŸç”Ÿ PyTorch é«˜ 2-3 å€ã€‚  

#### æ˜“ç”¨æ€§
- **ç»Ÿä¸€æ¥å£**ï¼šæä¾›å‘½ä»¤è¡Œå·¥å…·å’Œé…ç½®æ–‡ä»¶ï¼Œæ— éœ€ç¼–å†™å¤æ‚è®­ç»ƒä»£ç ã€‚  
  ```bash
  # ç¤ºä¾‹ï¼šä½¿ç”¨ LoRA å¾®è°ƒ LLaMA-7B
  python src/train_bash.py \
    --model_name_or_path meta-llama/Llama-2-7b-hf \
    --dataset alpaca_gpt4_zh \
    --lora_rank 8 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4
  ```
- **å¯è§†åŒ–ç›‘æ§**ï¼šé›†æˆ TensorBoard å’Œ WandB æ—¥å¿—è®°å½•ã€‚  

#### çµæ´»æ€§
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šå¯æ›¿æ¢æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥æ¨¡å—ã€‚  
- **å¤šé˜¶æ®µè®­ç»ƒ**ï¼šæ”¯æŒé¢„è®­ç»ƒï¼ˆPretrainï¼‰â†’ æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰â†’ å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å…¨æµç¨‹ã€‚   


###  ä¸åŒç±»æ¡†æ¶å¯¹æ¯” 
| **æ¡†æ¶**                | **æ ¸å¿ƒä¼˜åŠ¿**                  | **é€‚ç”¨åœºæ™¯**      |     |
| --------------------- | ------------------------- | ------------- | --- |
| **Llama Factory**     | ä¸“ç²¾ LLaMA ç³»åˆ—ï¼Œè½»é‡åŒ–ã€æ˜“é…ç½®ï¼Œæœ‰ä¸­æ–‡æ–‡æ¡£ | ä¸­å°è§„æ¨¡æŒ‡ä»¤å¾®è°ƒã€é¢†åŸŸé€‚é… |     |
| **Hugging Face PEFT** | é€šç”¨ PEFT æ–¹æ³•æ”¯æŒï¼Œç”Ÿæ€å®Œå–„         | å¤šæ¨¡å‹æ¶æ„çš„è½»é‡å¾®è°ƒ    |     |
| **DeepSpeed**         | è¶…å¤§è§„æ¨¡å…¨é‡å¾®è°ƒï¼Œæ˜¾å­˜ä¼˜åŒ–             | åƒäº¿çº§æ¨¡å‹å·¥ä¸šçº§è®­ç»ƒ    |     |

### å¿«é€Ÿå…¥é—¨
#### ç¯å¢ƒå®‰è£…
```bash
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -r requirements.txt
```

#### æ•°æ®å‡†å¤‡
- ä½¿ç”¨é¢„ç½®æ•°æ®é›†æˆ–æŒ‰æ ¼å¼å‡†å¤‡è‡ªå®šä¹‰æ•°æ®ï¼š  
  ```json
  // data/custom_dataset.json
  [{"instruction": "...", "input": "...", "output": "..."}]
  ```

#### å¯åŠ¨è®­ç»ƒ
```bash
# å•å¡ QLoRA å¾®è°ƒ
python src/train_bash.py \
  --model_name_or_path meta-llama/Llama-2-7b-hf \
  --dataset custom_dataset \
  --quantization_bit 4 \
  --lora_rank 8 \
  --output_dir outputs
```

#### æ¨¡å‹åˆå¹¶ä¸æ¨ç†
```python
from transformers import AutoModelForCausalLM
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
model = PeftModel.from_pretrained(base_model, "outputs/checkpoint-1000")
model = model.merge_and_unload()  # åˆå¹¶ LoRA æƒé‡
model.save_pretrained("merged_model")
```


### é¡¹ç›®ç”Ÿæ€ä¸ç¤¾åŒº
- **æ´»è·ƒç¤¾åŒº**ï¼šGitHub 2k+ Starsï¼ŒæŒç»­æ›´æ–°æ–°æ¨¡å‹ï¼ˆå¦‚ LLaMA-3ã€DeepSeekï¼‰æ”¯æŒã€‚  
- **ä¼ä¸šåº”ç”¨**ï¼šå·²ç”¨äºæ•™è‚²ã€å®¢æœã€å†…å®¹ç”Ÿæˆç­‰é¢†åŸŸçš„æ¨¡å‹å®šåˆ¶ã€‚  


### **æ€»ç»“**  
**Llama Factory** æ˜¯ LLaMA ç³»åˆ—æ¨¡å‹è½»é‡çº§å¾®è°ƒçš„é¦–é€‰å·¥å…·ï¼Œå°¤å…¶é€‚åˆï¼š  
- **èµ„æºæœ‰é™**ï¼šæ¶ˆè´¹çº§ GPU å¾®è°ƒ 7B-13B æ¨¡å‹ã€‚  
- **å¿«é€Ÿè¿­ä»£**ï¼šé€šè¿‡é¢„ç½®æ•°æ®é›†å’Œé…ç½®æ¨¡æ¿ç®€åŒ–å®éªŒæµç¨‹ã€‚  
- **ä¸­æ–‡ä¼˜åŒ–**ï¼šé’ˆå¯¹ä¸­æ–‡æŒ‡ä»¤æ•°æ®æä¾›ä¸“é¡¹æ”¯æŒã€‚  

å¯¹äºéœ€è¦å…¨é‡å¾®è°ƒåƒäº¿çº§æ¨¡å‹æˆ–å¤æ‚åˆ†å¸ƒå¼è®­ç»ƒçš„å›¢é˜Ÿï¼Œå»ºè®®ç»“åˆ **DeepSpeed** æˆ– **Colossal-AI** ä½¿ç”¨ã€‚

# HuggingFaceå¾®è°ƒ

## Loraå¾®è°ƒ

**LoRA**(Low-Rank Adaptation)æ˜¯ä¸€ç§é«˜æ•ˆçš„å¤§æ¨¡å‹**PEFT**å¾®è°ƒæŠ€æœ¯ï¼Œå®ƒæ˜¯é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„å…³é”®å±‚ï¼ˆå¦‚å…¨è¿æ¥å±‚å’Œè‡ªæ³¨æ„åŠ›å±‚ï¼‰ä¹‹é—´æ·»åŠ ä½ç§©çŸ©é˜µæ¥å®Œæˆå¾®è°ƒã€‚è¿™äº›ä½ç§©çŸ©é˜µçš„å¼•å…¥ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€æ”¹å˜åŸæœ‰çš„å¤§é‡å‚æ•°ã€‚ç”±äºä½ç§©çŸ©é˜µçš„å‚æ•°æ•°é‡è¿œå°äºåŸæœ‰å±‚çš„å‚æ•°æ•°é‡ï¼Œè¿™å°±å¤§å¤§å‡å°‘äº†éœ€è¦è®­ç»ƒçš„å‚æ•°æ€»æ•°ã€‚
æ›´å¤šç»†èŠ‚å‚è€ƒ
* [æ·±å…¥æµ…å‡ºLora](https://zhuanlan.zhihu.com/p/650197598)
* [ä½é…æœºå™¨ä¹Ÿèƒ½å¾®è°ƒå¤§æ¨¡å‹ï¼Ÿæ‰‹æŠŠæ‰‹æ•™ä½ å®æˆ˜æŠ€å·§ï¼](https://mp.weixin.qq.com/s?__biz=MzA3NzE0MjAwMg==&mid=2452715024&idx=1&sn=4917ef9937647cd9bdb6d266d2f33620&chksm=89dcca5eb8513662d3f2d5e9ca366b10b12630f70b464c405139b3f72b738ec9bf29b3295f3e#rd)
* [åŸºäº Qwen2 å¤§æ¨¡å‹å¾®è°ƒæŠ€æœ¯è¯¦ç»†æ•™ç¨‹ï¼ˆLoRA å‚æ•°é«˜æ•ˆå¾®è°ƒå’Œ SwanLab å¯è§†åŒ–ç›‘æ§ï¼‰](https://www.cnblogs.com/obullxl/p/18312594/NTopic2024071801)
* [ä»é›¶å¼€å§‹æ‰‹å†™å¾®è°ƒä»£ç ï¼šå¦‚ä½•ç”¨æœ¬åœ°deepseekæ¨¡å‹åœ¨è‡ªå·±æ„å»ºçš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒ](https://mp.weixin.qq.com/s?__biz=MzU0NDkyMzg3Mg==&mid=2247483835&idx=1&sn=2e010f62f98ccde903ba81262e252bdb&chksm=fa5b6d53e3d525992829812cc4ea7653251cdb564b0752af69e54de098397380fc4c1143aacf#rd)
* [DeepSeekå¤§æ¨¡å‹å¾®è°ƒï¼Œå®¶æ•™å¼å…¨æµç¨‹å®æˆ˜æŒ‡å—ï¼](https://mp.weixin.qq.com/s?__biz=MzU3Nzc0MzE3MA==&mid=2247484640&idx=1&sn=ea0605378dc056460379c8bedcfa773f&chksm=fc12b7922060120563085c0111b74b6555d91887ebf365bf8d3db1ae1a2d91fe0cd6b018b923#rd)
* [æ‰‹æŠŠæ‰‹æ•™å­¦ï¼ŒDeepSeek-R1å¾®è°ƒå…¨æµç¨‹æ‹†è§£](https://www.cnblogs.com/shanren/p/18707513)
* [å¤§è¯­è¨€æ¨¡å‹å¸¸ç”¨å¾®è°ƒä¸åŸºäºSFTå¾®è°ƒDeepSeek R1æŒ‡å—](https://www.ewbang.com/community/article/details/1000168010.html)

### DeepSeek

#### åŠ è½½æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

model_name = "deepseek-ai/deepseek-llm-7b-base"
# Configure 4-bit quantization
# ä½¿ç”¨ 4 ä½é‡åŒ–ä½¿å¤§å‹æ¨¡å‹ä¸æœ‰é™çš„ GPU å†…å­˜å…¼å®¹ï¼š
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16  # Use float16 for faster computation
)
# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    quantization_config=bnb_config, 
    device_map="auto"
)
```

#### åŠ è½½æ•°æ®é›†

ä½¿ç”¨ IMDB æ•°æ®é›†å¯¹ DeepSeek LLM è¿›è¡Œæƒ…ç»ªåˆ†ç±»å¾®è°ƒï¼ŒåŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ï¼Œå¹¶å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†

```python

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("imdb")

def tokenize_function(examples):
    inputs = tokenizer(
        examples["text"], 
        truncation=True, 
        padding="max_length", 
        max_length=512
    )
    inputs["labels"] = inputs["input_ids"].copy()
    return inputs

# é¢„å¤„ç†æ•°æ®é›†
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(500))
small_test_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(100))
# Print a sample tokenized entry
print("Tokenized Sample:")
print(small_train_dataset[0])

```

#### é…ç½®Lora
```python
# Apply LoRA for memory-efficient fine-tuning
lora_config = LoraConfig(
    r=8,  # Low-rank adaptation size
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Apply LoRA to attention layers
    lora_dropout=0.05,
    bias="none"
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

#### è®¾ç½®è®­ç»ƒå‚æ•°
```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-4,  # Lower learning rate for LoRA fine-tuning
    per_device_train_batch_size=1,  # Reduce batch size for memory efficiency
    gradient_accumulation_steps=8,  # Simulate larger batch size
    num_train_epochs=0.5,
    weight_decay=0.01,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=50,
    fp16=True,  # Mixed precision training
)
```

#### åˆå§‹åŒ–è®­ç»ƒæœºå™¨å¹¶å¾®è°ƒè¯•
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_test_dataset,
)
print("ğŸš€ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

print("ğŸš€ å¼€å§‹å¾®è°ƒ")
trainer.train()


trainer.save_model("./my_deepseek")
tokenizer.save_pretrained("./my_deepseek")
print("æ¨¡å‹å·²ä¿å­˜åˆ° ./my_deepseek")
```

### Qwen2
#### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨(ç¡®è®¤åŸºåº§æ¨¡å‹)

```python
# æ¨¡å‹è·¯å¾„
TARGET_MODEL = ''

# ç¡®è®¤è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# é…ç½®8bité‡åŒ–ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘å¯¹è®¾å¤‡æ€§èƒ½çš„æ¶ˆè€—ã€‚
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
)

# åŠ è½½åˆ†è¯æœºå™¨
tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL)
tokenizer.padding_side = 'right'
tokenizer.pad_token = tokenizer.eos_token  # æ·»åŠ è¿™ä¸€è¡Œï¼Œè®¾ç½® pad_token

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    TARGET_MODEL,
    quantization_config=quantization_config,
    device_map=device_map
)
```

#### åŠ è½½æ•°æ®é›†(æ•°æ®å‡†å¤‡)

```python
# åŠ è½½æ•°æ®é›†
DATASET_NAME = ''
dataset = load_dataset(DATASET_NAME, trust_remote_code=True)
```

å¦‚æœæ•°æ®é›†ä¸­æ²¡æœ‰éªŒè¯é›†ï¼Œéœ€è¦æ‰‹åŠ¨åˆ’åˆ†
```python
# æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯é›†ï¼Œè‹¥æ²¡æœ‰åˆ™æ‰‹åŠ¨åˆ’åˆ†
if 'validation' not in dataset:
    # è¿™æ˜¯ datasets åº“ä¸­çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
    # test_size=0.1ï¼šè¡¨ç¤ºå°†æ•°æ®é›†çš„ 10% åˆ†é…ç»™æµ‹è¯•é›†ï¼ˆtestï¼‰ï¼Œå‰©ä¸‹çš„ 90% ä½œä¸ºè®­ç»ƒé›†ï¼ˆtrainï¼‰
    # shuffle = Trueï¼š   è¡¨ç¤ºåœ¨åˆ’åˆ†ä¹‹å‰å¯¹æ•°æ®è¿›è¡Œéšæœºæ‰“ä¹±ï¼Œä»¥ç¡®ä¿æ•°æ®åˆ†å¸ƒçš„éšæœºæ€§ã€‚
    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç æ—¶åˆ’åˆ†ç»“æœçš„ä¸€è‡´æ€§ã€‚
    split_dataset  = dataset["train"].train_test_split(test_size=0.1, shuffle=True, seed=42)
    dataset["train"] = split_dataset ["train"]
    # å°†é‡æ–°åˆ’åˆ†åçš„æµ‹è¯•é›† dataset["test"] é‡å‘½åä¸ºéªŒè¯é›† dataset["validation"]ï¼Œä»¥ä¾¿åœ¨åç»­ä»£ç ä¸­æ˜ç¡®åŒºåˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç”¨é€”ã€‚
    dataset["validation"] = split_dataset ["test"]
```

#### é…ç½®Lora(ç¡®è®¤å¾®è°ƒæ–¹æ³•)

è‡³å…³é‡è¦çš„ä¸€æ­¥
```python
# åˆ›å»ºLoRAé…ç½®
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,  # è®­ç»ƒæ¨¡å¼
    r=8,  # Lora ç§©
    lora_alpha=32,  # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†
    lora_dropout=0.1,  # Dropout æ¯”ä¾‹
)
 
# å°†LoRAåº”ç”¨äºæ¨¡å‹
model = get_peft_model(model, config)
 
```

#### è®­ç»ƒé…ç½®
```python
# åˆ›å»ºå¾®è°ƒå‚æ•°
args = TrainingArguments(
    output_dir=os.path.join(BASE_DIR, 'output', 'Qwen2-0.5B'),
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    num_train_epochs=2,
    save_steps=100,
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True,
    report_to="none",
)

# ç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡ï¼Œè®­ç»ƒç»“æŸå
plot_callback = PlotLossCallback()
# åˆ›å»ºè®­ç»ƒå™¨æ¢°
trainer = Trainer(
    model=model,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    peft_config=config,
    tokenizer=tokenizer,
    args=training_args,
    data_collator=data_collator,
    callbacks=[plot_callback]
)
# å¼€å§‹è®­ç»ƒ
trainer.train()
# ä¿å­˜ LoRA æƒé‡
model.save_pretrained("./fine_tuned_model")

```


